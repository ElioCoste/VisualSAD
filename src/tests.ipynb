{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ea3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AVADataset, AVADataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ee4619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cWYJHb25EVs:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cWYJHb25EVs: 100%|██████████| 15/15 [00:00<00:00, 68.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Kb1fduj-jdY: 100%|██████████| 15/15 [00:00<00:00, 183.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Creating dataset val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing P60OxWahxBQ:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing P60OxWahxBQ: 100%|██████████| 15/15 [00:00<00:00, 213.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yn9WN9lsHRE: 100%|██████████| 15/15 [00:00<00:00, 299.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Creating dataset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sntyb4omSfU:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sntyb4omSfU: 100%|██████████| 15/15 [00:00<00:00, 206.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tj-VmrMYtUI: 100%|██████████| 12/12 [00:00<00:00, 250.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n",
      "Dataframes already exist, skipping processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = AVADataset(mode=\"train\")\n",
    "val_dataset = AVADataset(mode=\"val\")\n",
    "test_dataset = AVADataset(mode=\"test\")\n",
    "\n",
    "train_loader = AVADataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = AVADataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = AVADataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73981a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(838, 765, 476)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e05a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "audio, visual, targets = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from VisualSAD import AudioEncoder, VisualEncoder, AudioVisualFusion, TemporalModel, ContrastiveLoss\n",
    "\n",
    "class VisualSAD(nn.Module):\n",
    "    \"\"\"\n",
    "    Main model class for VisualSAD.\n",
    "    Input shape is (batch_size, max_n_speakers, C, T, H, W) for visual data\n",
    "    and (batch_size, 4*T, C_mel) for audio data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 C, T, H, W, C_mel,\n",
    "                 embedding_dim_audio,\n",
    "                 embedding_dim_visual,\n",
    "                 embedding_dim,\n",
    "                 target_dim,\n",
    "                 lmbda=0.3):\n",
    "        super(VisualSAD, self).__init__()\n",
    "\n",
    "        self.C = C  # Number of channels in the visual data\n",
    "        self.T = T  # Number of time frames in the visual data\n",
    "        self.H = H  # Height of the visual data\n",
    "        self.W = W  # Width of the visual data\n",
    "        self.C_mel = C_mel  # Number of mel bands in the audio data\n",
    "        self.embedding_dim_audio = embedding_dim_audio\n",
    "        self.embedding_dim_visual = embedding_dim_visual\n",
    "        self.embedding_dim = embedding_dim  # Dimension of the final embedding\n",
    "        self.target_dim = target_dim  # Number of classes for classification\n",
    "\n",
    "        # Lambda parameter to balance the influence of the contrastive loss\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "        input_shape_audio = (4 * T, C_mel)\n",
    "        input_shape_visual = (C, T, H, W)\n",
    "\n",
    "        self.audio_encoder = AudioEncoder(\n",
    "            input_shape=input_shape_audio,\n",
    "            output_dim=embedding_dim_audio)\n",
    "        self.visual_encoder = VisualEncoder(\n",
    "            input_shape=input_shape_visual,\n",
    "            output_dim=embedding_dim_visual)\n",
    "\n",
    "        self.fusion = AudioVisualFusion(\n",
    "            embedding_dim_audio=embedding_dim_audio,\n",
    "            embedding_dim_visual=embedding_dim_visual,\n",
    "            output_dim=embedding_dim)\n",
    "\n",
    "        self.temporal_model = TemporalModel(\n",
    "            input_dim=embedding_dim,\n",
    "            output_dim=target_dim)\n",
    "\n",
    "        # Contrastive loss function\n",
    "        self.contrastive_loss = ContrastiveLoss()\n",
    "        # Final loss function for classification of speakers\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, audio, visual, targets):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        batch_size = audio.size(0)\n",
    "        n_speakers = visual.size(1)\n",
    "\n",
    "        audio_embedding = self.audio_encoder(audio)\n",
    "\n",
    "        # Collapse the first two dimensions of the visual input\n",
    "        # (batch_size * n_speakers, C, T, H, W)\n",
    "        visual = visual.view(-1, self.C, self.T, self.H, self.W)\n",
    "        visual_embedding = self.visual_encoder(visual)\n",
    "\n",
    "        # print(\n",
    "        #    f\"Audio embedding shape after encoder: {audio_embedding.shape}\")\n",
    "        # print(\n",
    "        #    f\"Visual embedding shape after encoder: {visual_embedding.shape}\")\n",
    "\n",
    "        # Compute the dot product of the audio and visual embeddings\n",
    "        # Broadcast the audio embedding to match the visual embedding shape\n",
    "        # Shape is (batch_size, n_speakers, T, embedding_dim_audio)\n",
    "        audio_embedding = audio_embedding.unsqueeze(1)\n",
    "        audio_embedding = audio_embedding.expand(\n",
    "            -1, n_speakers, -1, -1)\n",
    "        # Collapse the first two dimensions of the audio embedding\n",
    "        # (batch_size * n_speakers, T, embedding_dim_audio)\n",
    "        audio_embedding = audio_embedding.contiguous().view(\n",
    "            -1, self.T, self.embedding_dim_audio)\n",
    "\n",
    "        print(\n",
    "           f\"Audio embedding shape after expanding: {audio_embedding.shape}\")\n",
    "        print(\n",
    "           f\"Visual embedding shape after reshaping: {visual_embedding.shape}\")\n",
    "        \n",
    "        # Select only the active frames i.e. the frames where at least one\n",
    "        # speaker is speaking (targets[..., t, :])\n",
    "        # and compute the contrastive loss using the\n",
    "        # corresponding embeddings\n",
    "        targets = targets.view(batch_size*n_speakers, self.T, -1)\n",
    "        \n",
    "        if self.training:\n",
    "            act_idx = targets.sum(dim=-1) > 0\n",
    "            if act_idx.sum() > 0:\n",
    "                act_embedding_audio = audio_embedding[act_idx]\n",
    "                act_embedding_visual = visual_embedding[act_idx]\n",
    "                print(\n",
    "                    f\"Active frames shape: {act_idx.shape}\")\n",
    "                print(\n",
    "                    f\"Active audio hape: {act_embedding_audio.shape}\")\n",
    "                print(\n",
    "                    f\"Active visual shape: {act_embedding_visual.shape}\")\n",
    "                # Shape is (batch_size * n_speakers, T_act, embedding_dim_audio)\n",
    "                # Compute the dot product along the 2 last dimensions\n",
    "                s = torch.matmul(\n",
    "                    act_embedding_audio, act_embedding_visual.transpose(1, 2))\n",
    "                # Compute the contrastive loss\n",
    "                loss_c = self.contrastive_loss(s)\n",
    "            else:\n",
    "                # If no active frames are present, set the loss to 0\n",
    "                loss_c = torch.tensor(0.0).to(audio.device)\n",
    "\n",
    "        # Apply the fusion module to get the final embedding\n",
    "        # Shape is (batch_size, T, embedding_dim)\n",
    "        x = self.fusion(audio_embedding, visual_embedding)\n",
    "\n",
    "        # Apply the temporal model to get the predictions\n",
    "        x = self.temporal_model(x)\n",
    "        x = x.view(batch_size*n_speakers, self.T, x.size(-1))\n",
    "        # print(f\"Shape after temporal model: {x.shape}\")\n",
    "\n",
    "        if self.training:\n",
    "            # Compute the classification loss\n",
    "            loss_cls = self.classification_loss(x.view(-1, x.size(-1)), targets.view(-1))\n",
    "            total_loss = loss_cls + self.lmbda * loss_c\n",
    "       \n",
    "        # print(f\"Targets shape: {targets.shape}\")\n",
    "        targets = targets.view(batch_size*n_speakers, self.T, -1)\n",
    "        loss_cls = self.classification_loss(x, targets)\n",
    "\n",
    "        # Reshape the output to (batch_size, n_speakers, output_dim)\n",
    "        if self.training:    \n",
    "            return total_loss, x.view(-1, x.size(1), x.size(2)), loss_cls, loss_c\n",
    "        return x.view(-1, x.size(1), x.size(2))\n",
    "    \n",
    "sad = VisualSAD(\n",
    "    C=1,\n",
    "    T=32,\n",
    "    H=112,\n",
    "    W=112,\n",
    "    C_mel=64,\n",
    "    embedding_dim_audio=128,\n",
    "    embedding_dim_visual=128,\n",
    "    embedding_dim=128,\n",
    "    target_dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561abafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 461648812\n"
     ]
    }
   ],
   "source": [
    "# Get the number of trainable parameters in the model\n",
    "def get_n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {get_n_params(sad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e31be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embedding shape after expanding: torch.Size([1, 32, 128])\n",
      "Visual embedding shape after reshaping: torch.Size([1, 32, 128])\n",
      "Active frames shape: torch.Size([1, 32])\n",
      "Active frames shape: torch.Size([7, 128])\n",
      "Active frames shape: torch.Size([7, 128])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msad\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mVisualSAD.forward\u001b[39m\u001b[34m(self, audio, visual, targets)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    107\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActive frames shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_embedding_visual.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Shape is (batch_size * n_speakers, T_act, embedding_dim_audio)\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Compute the dot product along the 2 last dimensions\u001b[39;00m\n\u001b[32m    110\u001b[39m s = torch.matmul(\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     act_embedding_audio, \u001b[43mact_embedding_visual\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Compute the contrastive loss\u001b[39;00m\n\u001b[32m    113\u001b[39m loss_c = \u001b[38;5;28mself\u001b[39m.contrastive_loss(s)\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "sad(audio, visual, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a86b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
