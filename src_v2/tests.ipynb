{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d33b6b",
   "metadata": {},
   "source": [
    "# Visual features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c356616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling.backbone.fpn import build_resnet_fpn_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15300cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import CfgNode as CN\n",
    "from detectron2.layers import ShapeSpec\n",
    "\n",
    "\n",
    "resnet_cfg = {\n",
    "    \"MODEL\": {\n",
    "        \"FPN\": {\n",
    "            \"IN_FEATURES\": [\"res2\", \"res3\", \"res4\", \"res5\"],\n",
    "            \"OUT_CHANNELS\": 128,\n",
    "            \"NORM\": \"BN\",\n",
    "            \"FUSE_TYPE\": \"sum\",\n",
    "        },\n",
    "        \"BACKBONE\": {\n",
    "            \"FREEZE_AT\": 2,\n",
    "        },\n",
    "        \"RESNETS\": {\n",
    "                \"OUT_FEATURES\": [\"res2\", \"res3\", \"res4\", \"res5\"],\n",
    "                \"DEPTH\": 18,\n",
    "                \"NUM_GROUPS\": 1,\n",
    "                \"WIDTH_PER_GROUP\": 1,\n",
    "                \"STEM_OUT_CHANNELS\": 1,\n",
    "                \"RES2_OUT_CHANNELS\": 64,\n",
    "                \"STRIDE_IN_1X1\": True,\n",
    "                \"RES5_DILATION\": 1,\n",
    "                \"DEFORM_ON_PER_STAGE\": [],\n",
    "                \"DEFORM_MODULATED\": False,\n",
    "                \"DEFORM_NUM_GROUPS\": [],\n",
    "                \"NORM\": \"BN\"\n",
    "            }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a config object\n",
    "resnet_cfg = CN(resnet_cfg)\n",
    "\n",
    "\n",
    "B = 1\n",
    "T = 5\n",
    "C = 3\n",
    "W = 512\n",
    "H = 512\n",
    "\n",
    "input_shape_resnet = ShapeSpec(\n",
    "    channels=C,\n",
    "    height=H,\n",
    "    width=W,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn = build_resnet_fpn_backbone(resnet_cfg, input_shape_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ebf60930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in FPN: 11733760\n"
     ]
    }
   ],
   "source": [
    "# Print the number of parameters in the FPN\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in FPN: {count_parameters(fpn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51dd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.randn(B*T, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "422a8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fpn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "36b57bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b76c44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape everything to B, C, T, H, W\n",
    "for k, v in res.items():\n",
    "    res[k] = v.view(B, T, *v.shape[1:]).permute(0, 2, 1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "65977564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 5, 128, 128]),\n",
       " torch.Size([1, 128, 5, 64, 64]),\n",
       " torch.Size([1, 128, 5, 32, 32]),\n",
       " torch.Size([1, 128, 5, 16, 16]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['p2'].shape, res['p3'].shape, res['p4'].shape, res['p5'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e0a47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from detectron2.modeling.backbone.fpn import build_resnet_fpn_backbone\n",
    "\n",
    "from audio_encoder import AudioEncoder\n",
    "\n",
    "from tpavi import TPAVI\n",
    "\n",
    "\n",
    "class MainModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape_resnet,\n",
    "                 resnet_cfg,\n",
    "                 T\n",
    "                 ):\n",
    "        super(MainModel, self).__init__()\n",
    "        \n",
    "        self.T = T\n",
    "        self.W = input_shape_resnet.width\n",
    "        self.H = input_shape_resnet.height\n",
    "        self.C = input_shape_resnet.channels\n",
    "        \n",
    "        # Initialize the visual encoder (ResNet18 + FPN model)\n",
    "        self.visual_encoder = build_resnet_fpn_backbone(resnet_cfg, input_shape_resnet)\n",
    "\n",
    "        # Initialize the audio encoder\n",
    "        self.audio_encoder = AudioEncoder()\n",
    "        self.dim_audio = 128  # Constant defined in the AudioEncoder block\n",
    "\n",
    "        # Initialize the fusion modules (TPAVI) for each feature map\n",
    "        # ouput of the visual encoder\n",
    "        self.fusion_modules = nn.ModuleList()\n",
    "        for feature_map in self.visual_encoder.output_shape().values():\n",
    "            self.fusion_modules.append(\n",
    "                TPAVI(\n",
    "                    C=feature_map.channels,\n",
    "                    T=self.T,\n",
    "                    dim_audio=self.dim_audio,\n",
    "                ))\n",
    "\n",
    "    def forward_audio_encoder(self, audio):\n",
    "        \"\"\"\n",
    "        Forward pass of the audio encoder.\n",
    "\n",
    "        Args:\n",
    "            audio (torch.Tensor): Audio input of shape (B, 4T, N_MFCC).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature tensor of shape (B, T, 128).\n",
    "        \"\"\"\n",
    "        audio = audio.unsqueeze(1).transpose(-1, -2)\n",
    "        return self.audio_encoder(audio)\n",
    "\n",
    "    def forward_visual_encoder(self, video):\n",
    "        \"\"\"\n",
    "        Forward pass of the visual encoder.\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor): Video input of shape (B, T, C, W, H).\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing feature maps from the visual encoder.\n",
    "        \"\"\"\n",
    "        # Change shape to (B*T, C, W, H)\n",
    "        print(video.shape)\n",
    "        video = video.view(\n",
    "            video.size(0)*video.size(1), *video.size()[2:])\n",
    "        print(video.shape)\n",
    "        return self.visual_encoder(video)\n",
    "\n",
    "\n",
    "    def forward(self, audio, video):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            audio (torch.Tensor): Audio input of shape (B, 4T, N_MFCC).\n",
    "            video (torch.Tensor): Video input of shape (B, T, C, W, H).\n",
    "        \"\"\"\n",
    "        visual_features = self.forward_visual_encoder(video)\n",
    "        audio_features = self.forward_audio_encoder(audio)\n",
    "        print(audio_features.shape)\n",
    "        return visual_features, audio_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c1de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 13, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio = torch.randn(B, 4*T, 13)\n",
    "audio = audio.unsqueeze(1).transpose(-1, -2)\n",
    "print(audio.shape)\n",
    "model.audio_encoder(audio).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14098e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3, 256, 256])\n",
      "torch.Size([5, 3, 256, 256])\n",
      "torch.Size([1, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "from config import cfg, input_shape\n",
    "from utils import T, C\n",
    "from detectron2.layers import ShapeSpec\n",
    "\n",
    "B = 1\n",
    "T = 5\n",
    "\n",
    "input_shape = ShapeSpec(\n",
    "    channels=C,\n",
    "    height=256,\n",
    "    width=256,\n",
    ")\n",
    "\n",
    "model = MainModel(\n",
    "    input_shape_resnet=input_shape,\n",
    "    resnet_cfg=cfg,\n",
    "    T=T\n",
    ")\n",
    "\n",
    "visual_features, audio_features = model(\n",
    "    audio=torch.randn(B, 4*T, 13),\n",
    "    video=torch.randn(B, T, C, 256, 256),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36dde6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
