{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d33b6b",
   "metadata": {},
   "source": [
    "# Visual features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0a47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_model import MainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14098e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python312\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval output shape: torch.Size([10, 25, 6])\n"
     ]
    }
   ],
   "source": [
    "from config import resnet_cfg, resnet_input_shape, args\n",
    "from config import T, C, NUM_CLASSES, N_MFCC\n",
    "from detectron2.layers import ShapeSpec\n",
    "import torch\n",
    "\n",
    "B = 5\n",
    "T = 2\n",
    "\n",
    "resnet_input_shape = ShapeSpec(\n",
    "    channels=C,\n",
    "    height=128,\n",
    "    width=128,\n",
    ")\n",
    "\n",
    "model = MainModel(\n",
    "    input_shape_resnet=resnet_input_shape,\n",
    "    cfg_resnet=resnet_cfg,\n",
    "    args=args,\n",
    "    T=T,\n",
    "    N_MFCC=N_MFCC,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "out_eval = model(\n",
    "    torch.randn(B, 4*T, 13),  # audio\n",
    "    torch.randn(B, T, C, 128, 128),  # visual\n",
    ")\n",
    "\n",
    "print(\"Eval output shape:\", out_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76075d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output shape: [torch.Size([10, 67, 32, 32]), torch.Size([10, 67, 16, 16]), torch.Size([10, 67, 8, 8]), torch.Size([10, 67, 4, 4]), torch.Size([10, 67, 2, 2])]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "audio_input = torch.randn(B, 4*T, 13)\n",
    "visual_input = torch.randn(B, T, C, 128, 128)\n",
    "\n",
    "visual_features = model.forward_visual_encoder(visual_input)\n",
    "audio_features = model.forward_audio_encoder(audio_input)\n",
    "fused_features = model.forward_fusion(visual_features, audio_features)\n",
    "head_output = model.forward_head(fused_features)\n",
    "\n",
    "out_train = model.forward_head(\n",
    "    fused_features\n",
    ")\n",
    "\n",
    "print(\"Train output shape:\", [x.shape for x in out_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52657a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import v8DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6949a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_loss = v8DetectionLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4839deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_batch = {\n",
    "    \"batch_idx\": torch.arange(B),\n",
    "    \"cls\": torch.randint(0, NUM_CLASSES, (B,)),\n",
    "    \"bboxes\": torch.rand(B, 4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a5a2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([10, 67, 32, 32]),\n",
       " torch.Size([10, 67, 16, 16]),\n",
       " torch.Size([10, 67, 8, 8]),\n",
       " torch.Size([10, 67, 4, 4]),\n",
       " torch.Size([10, 67, 2, 2])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in out_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7136b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4507e+00, 9.0152e+03, 1.4841e+01], grad_fn=<MulBackward0>),\n",
       " tensor([1.4507e-01, 9.0152e+02, 1.4841e+00]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_loss(out_train, targets_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dac3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import resnet_cfg, resnet_input_shape, T, C, NUM_CLASSES, N_MFCC, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AVADataset, AVADataLoader\n",
    "\n",
    "\n",
    "train_dataset = AVADataset(\n",
    "    \"train\", N_MFCC,\n",
    "    C, H, W, T\n",
    ")\n",
    "    \n",
    "train_loader = AVADataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33041825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python312\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from main_model import MainModel\n",
    "\n",
    "T = 2\n",
    "H = 128\n",
    "W = 128\n",
    "\n",
    "model = MainModel(\n",
    "    input_shape_resnet=resnet_input_shape,\n",
    "    cfg_resnet=resnet_cfg,\n",
    "    T=T,\n",
    "    N_MFCC=N_MFCC,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5295893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13428367"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578cf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (audio, visual, targets, boxes) in enumerate(train_loader):\n",
    "#     print(\"Batch\", i)\n",
    "#     print(\"Audio shape:\", audio.shape)\n",
    "#     print(\"Visual shape:\", visual.shape)\n",
    "#     print(\"Targets shape:\", targets.shape)\n",
    "#     print(\"Boxes shape:\", boxes.shape)\n",
    "\n",
    "#     if i == 1:\n",
    "#         break\n",
    "import torch\n",
    "\n",
    "audio = torch.randn(1, 4*T, 13)\n",
    "visual = torch.randn(1, T, C, H, W)\n",
    "targets = torch.randint(0, NUM_CLASSES, (1, T, 2))\n",
    "boxes = torch.rand(1, T, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913a21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e6ba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 13]),\n",
       " torch.Size([1, 2, 3, 128, 128]),\n",
       " torch.Size([1, 2, 2]),\n",
       " torch.Size([1, 2, 2, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape, visual.shape, targets.shape, boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "550b7bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 128]),\n",
       " [torch.Size([1, 2, 128, 32, 32]),\n",
       "  torch.Size([1, 2, 128, 16, 16]),\n",
       "  torch.Size([1, 2, 128, 8, 8]),\n",
       "  torch.Size([1, 2, 128, 4, 4]),\n",
       "  torch.Size([1, 2, 128, 2, 2])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features = model.forward_audio_encoder(audio.to(device))\n",
    "visual_features = model.forward_visual_encoder(visual.to(device))\n",
    "\n",
    "audio_features.cpu().shape, [x.shape for x in visual_features.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb52d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 128]), torch.Size([1, 2, 128, 32, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa = audio_features\n",
    "xv = [x for x in visual_features.values()][0]\n",
    "\n",
    "xa.shape, xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio is (B, T, 128)\n",
    "# Each visual feature map is (B, T, 128, h_i*w_i)\n",
    "\n",
    "# Audio-visual fusion: \n",
    "# 1. Use convolution to project visual features to the same dimension as audio features\n",
    "# 2. Apply cross attention\n",
    "# 3. Upsample fused features to the same spatial resolution as visual features\n",
    "# 4. Add a residual connection to the original visual features\n",
    "\n",
    "\n",
    "# 1. Convolution to project visual features to the same dimension as audio features\n",
    "xv = xv.view(xv.shape[0]*xv.shape[1], xv.shape[2], xv.shape[3], xv.shape[4])\n",
    "print(\"xv shape after view:\", xv.shape)\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=xv.shape[1],\n",
    "    out_channels=xa.shape[1],\n",
    "    kernel_size=1,\n",
    "    stride=1,\n",
    "    padding=0\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
